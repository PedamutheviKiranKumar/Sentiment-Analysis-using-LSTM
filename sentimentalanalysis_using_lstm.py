# -*- coding: utf-8 -*-
"""Sentimentalanalysis_using_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NOIXN2IPjfL43lWoLA2SQf72HnIQzUJS
"""

import tensorflow as tf
from tensorflow.keras import layers

import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
nltk.download('stopwords')

import gensim

import pandas as pd
import numpy as np
import re

import matplotlib.pyplot as plt


from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)s

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':'1Y15XHsyLLNX93XLYFcmBqo3vsOO57eRG'}) # replace the id with id of file you want to access
downloaded.GetContentFile('training.1600000.processed.noemoticon.csv')

df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin', header= None)

df.head()

df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']

df.head(2)

# drop all the olumns except the text column

df.drop(['date', 'id', 'query', 'user_id'], axis=1, inplace=True)

df.head(3)

df['sentiment'].value_counts().plot(kind='bar', title='sentiment')

lab_to_sentiment = {0:"Negative", 4:"Positive"}
def label_decoder(label):
    return lab_to_sentiment[label]
df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))
df.head()

df.sentiment.value_counts()

"""Text Preprocessing"""

stop_words = stopwords.words('english')
stemmer =  SnowballStemmer('english')

text_cleaning_re =  "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"

def preprocess(text, stem=False):
    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            if stem:
                tokens.append(stemmer.stem(token))
            else:
                tokens.append(token)
    return " ".join(tokens)

df['text'] = df.text.apply(lambda x : preprocess(x))

df.head()

from wordcloud import WordCloud

plt.figure(figsize = (20,20)) 
wc = WordCloud(max_words=2000, width = 1600, height = 800).generate(str(df[df.sentiment == 'Positive'].text))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20))

wc = WordCloud(max_words= 2000, height=800, width=1200).generate(' '.join(df[df['sentiment'] == 'Negative'].text))
plt.imshow(wc , interpolation = 'bilinear')

"""Train and Test Split"""

TRAIN_SIZE = 0.8
MAX_NB_WORDS = 100000
MAX_SEQUENCE_LENGTH = 30

train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE ,random_state=7) # Splits Dataset into Training and Testing set
print("Train Data size:", len(train_data))
print("Test Data size:", len(test_data))

"""Word Emdedding"""

document = [d.split() for d in train_data['text']]

word2vec_model = gensim.models.word2vec.Word2Vec(size= 300, window=7, workers=6, min_count=10)

word2vec_model.build_vocab(document)

word = word2vec_model.wv.vocab.keys()

vocab_size = len(word)

print("Vocab size", vocab_size)

word2vec_model.train(document, epochs=32, total_examples=len(document))

word2vec_model.most_similar('earth')

"""Tokenize"""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenize = Tokenizer()

tokenize.fit_on_texts(train_data['text'])
word_index = tokenize.word_index

vocab_size = len(tokenize.word_index) + 1
print("Vocabulary Size :", vocab_size)

"""Sequence of word"""

# create sequence of word
from tensorflow.keras.preprocessing.sequence import pad_sequences

x_train = pad_sequences(tokenize.texts_to_sequences(train_data.text), maxlen=MAX_SEQUENCE_LENGTH)

x_test = pad_sequences(tokenize.texts_to_sequences(test_data.text), maxlen=MAX_SEQUENCE_LENGTH)

x_train[1]

print("Training X Shape:",x_train.shape)
print("Testing X Shape:",x_test.shape)

labels = train_data.sentiment.unique().tolist()

"""Label Encoding"""

encoder = LabelEncoder()
# encoder.fit(train_data.sentiment.to_list())

y_train = encoder.fit_transform(train_data.sentiment.to_list())
y_test = encoder.fit_transform(test_data.sentiment.to_list())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

print("y_train shape:", y_train.shape)

print("y_test shape:", y_test.shape)

EMBEDDING_DIM = 300

LR = 1e-3

BATCH_SIZE = 1024

EPOCHS = 10

"""if error

-> !wget http://nlp.stanford.edu/data/glove.6B.zip

-> !unzip glove.6B.zip

->GLOVE_EMB = './glove.6B.300d.txt'

EMBEDDING_DIM = 300

LR = 1e-3

BATCH_SIZE = 1024

EPOCHS = 10

MODEL_PATH = '.../output/kaggle/working/best_model.hdf5'
"""

embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))

for word, i in word_index.items():
    if word in word2vec_model.wv:
        embedding_matrix[i] = word2vec_model.wv[word]
        
print(embedding_matrix.shape)

sequence_input = layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')

embedding_layer = layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], 
                                   input_length=MAX_SEQUENCE_LENGTH,
                                    trainable=False)(sequence_input)
x = layers.SpatialDropout1D(0.2)(embedding_layer)

x = layers.Conv1D(64, 5, activation='relu')(x)
x = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)

x = layers.Dense(512, activation='relu')(x)
x = layers.Dropout(0.5)(x)

x = layers.Dense(512, activation='relu')(x)

outputs = layers.Dense(1, activation='sigmoid')(x)

model = tf.keras.Model(sequence_input, outputs)

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',metrics=['accuracy'])

ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,
                                     min_lr = 0.01,
                                     monitor = 'val_loss',
                                     verbose = 1)

history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,
                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])

def decode_sentiment(score):
    return "Positive" if score>0.5 else "Negative"


scores = model.predict(x_test, verbose=1, batch_size=10000)
y_pred_1d = [decode_sentiment(score) for score in scores]

from sklearn.metrics import classification_report

print(classification_report(list(test_data.sentiment), y_pred_1d))